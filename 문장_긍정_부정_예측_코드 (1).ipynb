{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MtIKtW59B90",
        "outputId": "64ca6419-c102-4f6a-b3a2-40a531f7cfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6frCEVR8ViC",
        "outputId": "2ad67dc4-4d3c-48cb-a40e-a51b3236559b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-74add8a41e73>:26: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
            "<ipython-input-3-74add8a41e73>:27: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
            "<ipython-input-3-74add8a41e73>:31: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
            "<ipython-input-3-74add8a41e73>:32: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
            "100%|██████████| 145393/145393 [13:52<00:00, 174.63it/s]\n",
            "100%|██████████| 48852/48852 [05:07<00:00, 159.12it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:5071: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asarray(arr)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전체 샘플 중 길이가 30 이하인 샘플의 비율: 94.31944999380003\n",
            "Epoch 1/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.4110 - acc: 0.8088\n",
            "Epoch 1: val_acc improved from -inf to 0.83639, saving model to best_model.h5\n",
            "1815/1815 [==============================] - 202s 108ms/step - loss: 0.4110 - acc: 0.8088 - val_loss: 0.3709 - val_acc: 0.8364\n",
            "Epoch 2/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.3458 - acc: 0.8491\n",
            "Epoch 2: val_acc improved from 0.83639 to 0.84345, saving model to best_model.h5\n",
            "1815/1815 [==============================] - 180s 99ms/step - loss: 0.3458 - acc: 0.8491 - val_loss: 0.3579 - val_acc: 0.8435\n",
            "Epoch 3/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.3190 - acc: 0.8635\n",
            "Epoch 3: val_acc improved from 0.84345 to 0.85017, saving model to best_model.h5\n",
            "1815/1815 [==============================] - 175s 96ms/step - loss: 0.3190 - acc: 0.8635 - val_loss: 0.3415 - val_acc: 0.8502\n",
            "Epoch 4/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.2961 - acc: 0.8764\n",
            "Epoch 4: val_acc improved from 0.85017 to 0.85534, saving model to best_model.h5\n",
            "1815/1815 [==============================] - 192s 106ms/step - loss: 0.2961 - acc: 0.8764 - val_loss: 0.3385 - val_acc: 0.8553\n",
            "Epoch 5/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.2757 - acc: 0.8868\n",
            "Epoch 5: val_acc did not improve from 0.85534\n",
            "1815/1815 [==============================] - 178s 98ms/step - loss: 0.2757 - acc: 0.8868 - val_loss: 0.3441 - val_acc: 0.8542\n",
            "Epoch 6/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.2570 - acc: 0.8966\n",
            "Epoch 6: val_acc improved from 0.85534 to 0.85572, saving model to best_model.h5\n",
            "1815/1815 [==============================] - 192s 106ms/step - loss: 0.2570 - acc: 0.8966 - val_loss: 0.3378 - val_acc: 0.8557\n",
            "Epoch 7/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.2391 - acc: 0.9060\n",
            "Epoch 7: val_acc did not improve from 0.85572\n",
            "1815/1815 [==============================] - 196s 108ms/step - loss: 0.2391 - acc: 0.9060 - val_loss: 0.3565 - val_acc: 0.8497\n",
            "Epoch 8/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.2223 - acc: 0.9147\n",
            "Epoch 8: val_acc did not improve from 0.85572\n",
            "1815/1815 [==============================] - 182s 100ms/step - loss: 0.2223 - acc: 0.9147 - val_loss: 0.3509 - val_acc: 0.8509\n",
            "Epoch 9/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.2071 - acc: 0.9213\n",
            "Epoch 9: val_acc did not improve from 0.85572\n",
            "1815/1815 [==============================] - 191s 105ms/step - loss: 0.2071 - acc: 0.9213 - val_loss: 0.3693 - val_acc: 0.8504\n",
            "Epoch 10/15\n",
            "1815/1815 [==============================] - ETA: 0s - loss: 0.1916 - acc: 0.9284\n",
            "Epoch 10: val_acc did not improve from 0.85572\n",
            "1815/1815 [==============================] - 189s 104ms/step - loss: 0.1916 - acc: 0.9284 - val_loss: 0.3924 - val_acc: 0.8472\n",
            "Epoch 10: early stopping\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
        "\n",
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')\n",
        "\n",
        "# document 열과 label 열의 중복을 제외한 값의 개수\n",
        "train_data['document'].nunique(), train_data['label'].nunique()\n",
        "# document 열의 중복 제거\n",
        "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "train_data.loc[train_data.document.isnull()]\n",
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "\n",
        "# 한글과 공백을 제외하고 모두 제거\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
        "train_data['document'].replace('', np.nan, inplace=True)\n",
        "train_data = train_data.dropna(how = 'any')\n",
        "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거\n",
        "\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "okt = Okt()\n",
        "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)\n",
        "X_train = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(stopwords_removed_sentence)\n",
        "\n",
        "X_test = []\n",
        "for sentence in tqdm(test_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(stopwords_removed_sentence)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])\n",
        "\n",
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\n",
        "\n",
        "max_len = 30\n",
        "below_threshold_len(max_len, X_train)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ8WA7mhRm24",
        "outputId": "7e96b35d-0ef4-4239-9d59-26edffc05c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1527/1527 [==============================] - 36s 23ms/step - loss: 0.3476 - acc: 0.8525\n",
            "\n",
            " 테스트 정확도: 0.8525\n"
          ]
        }
      ],
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n",
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.5):\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDI-qRn0RuVn",
        "outputId": "01e9efb9-a42d-4acd-8105-9c67d1f406e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 453ms/step\n",
            "중립 문장입니다.\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "68.96% 확률로 긍정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "90.64% 확률로 긍정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "79.69% 확률로 긍정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "70.83% 확률로 긍정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "73.65% 확률로 부정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "74.11% 확률로 부정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "중립 문장입니다.\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "75.02% 확률로 부정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "중립 문장입니다.\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "85.04% 확률로 부정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "중립 문장입니다.\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "98.90% 확률로 부정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "96.16% 확률로 부정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "93.95% 확률로 긍정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "63.04% 확률로 긍정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "77.54% 확률로 부정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "중립 문장입니다.\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "93.98% 확률로 부정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "60.54% 확률로 부정 문장입니다.\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "83.95% 확률로 긍정 문장입니다.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.6):\n",
        "    print(\"{:.2f}% 확률로 긍정 문장입니다.\\n\".format(score * 100))\n",
        "  elif(score < 0.4):\n",
        "    print(\"{:.2f}% 확률로 부정 문장입니다.\\n\".format((1 - score) * 100))\n",
        "  elif(0.4<=score<=0.6):\n",
        "    print(\"중립 문장입니다.\")\n",
        "\n",
        "while(1):\n",
        "  answer= input(\"문장을 입력해주세요.\")\n",
        "  reply = sentiment_predict(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}